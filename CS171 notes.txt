CS171

What is Machine Learning?
- definition by Tom Mitchell: study of algorithms that: - improve their performance P, - at some task T, - with experience E

Traditional Programming
- data + program --> computer --> output

Machine Learning
- data + desired outputs (labels) --> computer --> program

When do we use ML?
- ML is used when human expertise does not exist (e.g. navigating on Mars)
- humans can't explain expertise (e.g. speech recognition)
- models must be customized (e.g. personalized medicine)
- models based on huge amounts of data (e.g. genomics)

examples of tasks best solved by learning algorithm
- recognizing patterns:
	: facial identities/expressions
	: handwritten or spoken words
	: medical images
- generating patterns:
	: generating text, images, motion sequences
- recognizing anomalies:
	: unusual credit card transactions
	: unusual patterns of sensor reading in a nuclear power plant
- prediction:
	: future stock prices
	: weather conditions

Sample Applications: web search, computational biology, finance, social networks, robotics, etc

Defining the Learning Task
- improve the task (T) with respect to performance metric (P) based on experience (E)
	e.g. T-playing checkers, P-% of games won, E-games against itself
	e.g. T-recognizing hand-written words, P-% of words correctly classified, E-database of images of hand-written words

Applications of ML
- autonomous cars
- facial recognition
- speech recognition
- generative AI

Types of Learning
- supervised learning: given training data + desired outputs (labels)

    Regression; given (x1,y1), (x2,y2)...(xn,yn)
	learn a function f(x) to predict y given x
	- in regression: y is a real number
	i.e. given x, y could be some number (1,2,3...)

    Classification; given (x1,y1), (x2,y2)...(xn,yn)
	learn a function f(x) to predict y given x
	- in classification: y is categorical
	i.e. given x, y can be only certain categories not numbers
	- x can be multi dimensional (each dimension corresponding to an attribute)
	e.g. giving tumor size + age instead of just tumor size

- unsupervised learning: given training data (w/o desired outputs)
	: given x1, x2, ... xn (w/o labels)
	: output hidden structure behind the x's 
		(e.g. clustering, machine can see clusters are related despite no y)

    Applications
	: genomics; group individuals by genetic similarity
	: organize computng clusters
	: social network analysis
	: market segmentation
	: astronomical data analysis

- reinforcement learning: rewards from sequence of actions
	: given a sequence of states and actions with (delayed) rewards, output a policy
	    - policy is a mapping from states -> actions that tells you what to do in a given state

	Agent-Environment Interface
	- Environment receives action from Agent, Agent receives reward/state from Environment

- semi-supervised learning: given training data + some desired outputs

Designing a Learning System
- choose the training experience
- choose exactly what is to be learned
	i.e. the target function
- choose how to represent the target function
- choose a learning algo to infer the target function from the experience

Training v Test Set
- training set; subset to train a model
- test set; subset to test the trained model (unseen data)
	: both are usually drawn from the same distributed data

------------------------
Review:
- Machine Learning <p t e>: performance, task, experience

- 4 types of ML: 
	supervised: given training data + desired outputs (labels)
	unsupervised: given training data (w/o desired outputs)
	reinforcement: rewards from sequence of actions
	semi-supervised: given training data + some desired outputs

- 2 types of supervised learning:
	regression: y is a real number
	classification: y is categorical

- 2 types of unsupervised learning:
	clustering: group similar data points
	dimensionality reduction: reduce dimensionality of data

- training v test set: training set is used to train a model, test set is used to test the trained model
------------------------
Decision Trees (not usually used in production, industry uses decision tree: ensemble)
What is a Decision Tree?
- is a Supervised learning technique that can be used for both Classification and Regression problems (mostly preferred for Classification)
	e.g. Buying a car (decision tree)
		- is it a 2 door car?
			- yes: is it a sports car?
				- yes: buy it
				- no: don't buy it
			- no: is it a 4 door car?
				- yes: is it a luxury car?
					- yes: buy it
					- no: don't buy it
				- no: don't buy it

Tree Induction - challenges
1 how to classify a leaf node? 
	: assign the majority class (i.e. whichever side has the highest popularity)
2 determine how to split the records?
	: how to specify the attribute test condition?
		- depends on attribute types
			| nominal - there is no order to the values (e.g. red, green, blue)
			| ordinal - there is an order to the values (e.g. low, medium, high)
			| continuous - there is an infinite number of values (e.g. 1.2, 1.3, 1.4)
		- depends on number of ways to split
	: how to determine the best attribute to split on? (2 way/binary split | multi-way split)
		- nodes with homogeneous class distribution is preferred
		- need a measure of node impurity
			| entropy - shows randomness of data
			| gini index - shows purity of data 
			| information gain - shows how much information is gained by splitting data
3 determine when to stop splitting
	: when a node is 100% in one class
	: when splitting a node will result in the tree exceeding max depth
	: when info gain is less than threshold
	: when number of examples in node is below threshold
	: early termination

Pros 
	: inexpensive
	: extremely fast
	: easy to interpret
	: accuracy comparable to other methods

Cons
	: can create overly complex trees that do not generalize data well (overfitting)
	: highly sensitive to small changes of the data: high variance
		- small changes in data can result in large changes in tree
		- can be reduced by ensemble methods
------------------------
Review - Decision Trees
- how to determine attribute test condition?
	: attribute types, number of ways to split
- how to determine the best split?
	: homogeneous class distribution, measure of node impurity (entropy, gini index, information gain)

Bias - error between avg model prediction/truth (how well the model fits the data)

Variance - avg variability in model prediction for the dataset (how much the model changes based on the input data)

Bias-Variance Tradeoff
	: high bias, low variance
		- underfitting (overly simplified model)
		- high error on both test and train data
	: low bias, high variance
		- overfitting (overly complex model)
		- low error on train data, high error on test data
	: low bias, low variance
		- good fit
	: high bias, high variance
		- bad fit

Training and Test Errors
- training data: used to train the model
- test data: used to test the model
- test error is more important than training error (because we want to know how well the model generalizes to unseen data)
	: we care abt generalization
	: i.e. training data w no errors seems good but the test data might have a lot of errors bc the model is
		 perfect for the training but not for generalized data sets
- Generalization Errors
	: Re-substition errors: error on training (e(t))
	: Generalization errors: error on test (e'(t))
- Methods for estimating Generalization Errors
	: Optimistic approach (e'(t))
		- use training error as an estimate of generalization error
		- e'(t) = e(t)
		- problem: training error is usually optimistic
	: Pessimistic approach (e'(t))
		- use training error + complexity penalty as an estimate of generalization error
		- e'(t) = e(t) + complexity penalty
		- problem: complexity penalty is usually pessimistic
		e.g. given penality is 0.5
			Training error 10/1000 = 1%
			Generalization error (10 + 30 x 0.5)/1000 = 2.5%
		Optimistic approach (just training error) is too optimistic
		Pessimistic approach (Generalization error) is more realistic
	: Reduced error pruning (REP)
		- split training data into 2 components
			| training set - used to train the model
			| validation set - used to estimate generalization error

Overfitting
- model fits the training data too well
	: model is too complex
	: model is too sensitive to small variations in training data
	: model does not generalize well to test data
- pre-pruning
	: stop growing the tree when some stopping criteria is met
	: e.g. stop growing the tree when the number of examples in a node is below a threshold
- post-pruning
	: grow the tree to completion, then prune the tree
	: e.g. if removing a sub-tree improves generalization error (reduces), then replace sub-tree with leaf node
		- the class label of that leaf node is the majority class of the sub-tree

Model Evaluation in Classification
- metrics for Performance Evaluation
	: predictive capability - how well does the model predict the class label of unseen data?
		- confusion matrix
			| TP - true positive 
			| TN - true negative 
			| FP - false positive (model predicts positive but is actually negative)
			| FN - false negative (model predicts negative but is actually positive)
	: accuracy
		- fraction of correct predictions
		- accuracy = (TP + TN)/(TP + TN + FP + FN) == (true predictions)/(total predictions)
		- can be misleading, e.g. if 99% of data is class A, then a model that always predicts class A will have 99% accuracy
	: precision
		- fraction of positive predictions that are correct
		- precision = TP/(TP + FP) == (true positive predictions)/(total positive predictions)
	: recall
		- fraction of positive examples that are correctly predicted
		- recall = TP/(TP + FN) == (true positive predictions)/(total positive examples)
	: F1 - measure = 2 * (precision * recall)/(precision + recall)
	: ROC curve
		- plots true positive rate (TPR) vs false positive rate (FPR)
- methods for Performance Evaluation
	: estimation
		- holdout method (2/3 data for training, 1/3 data for testing)
			| split data into training and test sets
			| train model on training set
			| evaluate model on test set
		- random subsampling 
			| repeated holdout method
		- cross validation (k-fold cross validation)
			| split data into training and testing
			| training is broken into k equal sized subsets
			| e.g. fold 1 is for validation, fold 2-5 is for training, then fold 2 is for validation, fold 1,3-5 is for training, etc
				- imagine validation data as a mini-test data inside the training data	
			| repeat k times, each time using a different subset for testing
		- bootstrap
			| randomly sample data with replacement
			| train model on sampled data
			| evaluate model on unsampled data

------------------------------------------------
Ensemble Learning
- weak learner/classifier - learning algo that performs slighly better than chance (e.g. correctly labels ~.6)

(Accuracy v Efficiency -------------------------)
| accuracy - true predictions/total predictions |
| efficiency - complexity, time, space			|
(-----------------------------------------------)

- Why use Ensemble Learning?
	: can combine weak learners to create a strong learner
	: basic ensemble functions
		- max voting (classification - discrete)
			| class with the most votes is the predicted class
		- averaging (regression - continuous)
			| average of all predictions is the predicted value
		- weighted averaging (regression)
			| each learner is given a certain weight and thus higher weight have more influence on the final prediction

- What is bagging? (bootstrap aggregating)
	: multiple weak learners are trained in parallel (each learner is independent of each other)
	: each weak learner has input data randomly sampled from the original dataset with replacement
	: final prediction is the average of all weak learners
	: reduces variance (how much the model changes based on input data)
	: reduces overfitting (model fits training data too well)

- What is boosting?
	: multiple weak learners are trained in sequence (each learner is dependent on each other)
		- think of it as a chain of weak learners where each sequential learner has more information than the previous learner (weights of mistakes from prior learner)
	: each subsequent model is trained by giving more weight to examples that were misclassified by previous models
		- e.g. if model 1 misclassified 10 examples, then model 2 will give more weight to those 10 examples
	: final prediction is chosen by scaling each weak-learner's prediction by its weight (why give weight to wrong answers?)
	: reduces bias (error between avg model prediction/truth - how good the model is at predicting the data)

- What is stacking?
	: multiple weak learners (different types) are trained in parallel (each learner is independent of each other)
	: does not use simple voting for final prediction
	: creates a meta learner trained on the outputs of the weak learners
	: 2 subsets of the training data
		- one for training the weak learners
		- one for training the meta learner (weak learners are fed the data then the output is fed to meta learner)

Summary of Ensemble Methods (you can combine based on the problem)
	: bagging - reduces variance, simple voting
	: boosting - reduces bias, weighted voting
	: stacking - improves accuracy, learned voting	

Decision Tree Ensemble Methods
	: issues with decision trees - (over/under fitting)
	Random Forest
		- bagging algo on decision trees
			1. pick N random records from data set (bootstrap)
			2. build a decision tree based on the N records
			3. repeat steps 1-2 k times to make k trees
			4. each tree predicts the class label of the prediction, majority class wins
	Pros
		: used for classification & regression
		: works well w categorical/numerical data, no scaling or transformation required
		: generally provides high accuracy and balance the bias-variance trade off
	Cons
		: not easily interpretable
		: computationally intense
		: like "black box" algo, little control over what model does

	(Bootstrap v Cross Validation (kfold) -------------------------------------)
	| Bootstrap - sampling with replacement (can have overlap/reused data)	   |
	| Cross Validation - sampling without replacement (no overlap/reused data) |
	(--------------------------------------------------------------------------)

	Gradient boosting (how is it different from regular boosting?)
		- Boosting algo on decision trees
			: calculates loss function, updates model parameters in direction that reduces loss function the most
			: final prediction is obtained by adding up the projections of each individual tree
	Pros
		: used for classification & regression
		: helpful for managing complicated heterogenous data and enhancing model correctness
		: can handle categorical and continuous vars
		: high prediction accuracy 
	Cons
	: computationally expensive
	: overfitting

	(Heterogeneous v homogeneous Data set --------------------------------------)
	| Heterogeneous can have different types, e.g. numerical and categorical 	|
	| Homogeneous can only have one type, e.g. all numerical or all categorical |
	(---------------------------------------------------------------------------)

	XGBoost (extreme gradient boosting)
		- Boosting algo on decision trees
			: custom loss function 
			: main diff between Gradient and XG - uses regularization technique
		- performs better than Gradient bc of regularization
		- utilizes CPU parallel processing
		- 2 main components
			: tree boosting - builds trees one at a time, each tree is built to correct the errors of the previous tree
			: regularization - penalizes more complex models

	AdaBoost (Adaptive Boosting)
		- Decision Stumps (decision trees with only 1 split)
		- weak learner needs to minimize weighted error
		- more intuitive than Gradient but Gradient is more flexible (generic algo)
		- AdaBoost is slower bc needing multiple iterations to build the sequence of models
		- AdaBoost is more abt "voting weights" (?)
		- Gradient is more abt "adding gradient optimization" (?)
