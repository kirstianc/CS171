CS171

What is Machine Learning?
- definition by Tom Mitchell: study of algorithms that: - improve their performance P, - at some task T, - with experience E

Traditional Programming
- data + program --> computer --> output

Machine Learning
- data + desired outputs (labels) --> computer --> program

When do we use ML?
- ML is used when human expertise does not exist (e.g. navigating on Mars)
- humans can't explain expertise (e.g. speech recognition)
- models must be customized (e.g. personalized medicine)
- models based on huge amounts of data (e.g. genomics)

examples of tasks best solved by learning algorithm
- recognizing patterns:
	: facial identities/expressions
	: handwritten or spoken words
	: medical images
- generating patterns:
	: generating text, images, motion sequences
- recognizing anomalies:
	: unusual credit card transactions
	: unusual patterns of sensor reading in a nuclear power plant
- prediction:
	: future stock prices
	: weather conditions

Sample Applications: web search, computational biology, finance, social networks, robotics, etc

Defining the Learning Task
- improve the task (T) with respect to performance metric (P) based on experience (E)
	e.g. T-playing checkers, P-% of games won, E-games against itself
	e.g. T-recognizing hand-written words, P-% of words correctly classified, E-database of images of hand-written words

Applications of ML
- autonomous cars
- facial recognition
- speech recognition
- generative AI

Types of Learning
- supervised learning: given training data + desired outputs (labels)

    Regression; given (x1,y1), (x2,y2)...(xn,yn)
	learn a function f(x) to predict y given x
	- in regression: y is a real number
	i.e. given x, y could be some number (1,2,3...)

    Classification; given (x1,y1), (x2,y2)...(xn,yn)
	learn a function f(x) to predict y given x
	- in classification: y is categorical
	i.e. given x, y can be only certain categories not numbers
	- x can be multi dimensional (each dimension corresponding to an attribute)
	e.g. giving tumor size + age instead of just tumor size

- unsupervised learning: given training data (w/o desired outputs)
	: given x1, x2, ... xn (w/o labels)
	: output hidden structure behind the x's 
		(e.g. clustering, machine can see clusters are related despite no y)

    Applications
	: genomics; group individuals by genetic similarity
	: organize computng clusters
	: social network analysis
	: market segmentation
	: astronomical data analysis

- reinforcement learning: rewards from sequence of actions
	: given a sequence of states and actions with (delayed) rewards, output a policy
	    - policy is a mapping from states -> actions that tells you what to do in a given state

	Agent-Environment Interface
	- Environment receives action from Agent, Agent receives reward/state from Environment

- semi-supervised learning: given training data + some desired outputs

Designing a Learning System
- choose the training experience
- choose exactly what is to be learned
	i.e. the target function
- choose how to represent the target function
- choose a learning algo to infer the target function from the experience

Training v Test Set
- training set; subset to train a model
- test set; subset to test the trained model (unseen data)
	: both are usually drawn from the same distributed data

------------------------
Review:
- Machine Learning <p t e>: performance, task, experience

- 4 types of ML: 
	supervised: given training data + desired outputs (labels)
	unsupervised: given training data (w/o desired outputs)
	reinforcement: rewards from sequence of actions
	semi-supervised: given training data + some desired outputs

- 2 types of supervised learning:
	regression: y is a real number
	classification: y is categorical

- 2 types of unsupervised learning:
	clustering: group similar data points
	dimensionality reduction: reduce dimensionality of data

- training v test set: training set is used to train a model, test set is used to test the trained model
------------------------
Decision Trees (not usually used in production, industry uses decision tree: ensemble)
What is a Decision Tree?
- is a Supervised learning technique that can be used for both Classification and Regression problems (mostly preferred for Classification)
	e.g. Buying a car (decision tree)
		- is it a 2 door car?
			- yes: is it a sports car?
				- yes: buy it
				- no: don't buy it
			- no: is it a 4 door car?
				- yes: is it a luxury car?
					- yes: buy it
					- no: don't buy it
				- no: don't buy it

Tree Induction - challenges
1 how to classify a leaf node? 
	: assign the majority class (i.e. whichever side has the highest popularity)
2 determine how to split the records?
	: how to specify the attribute test condition?
		- depends on attribute types
			| nominal - there is no order to the values (e.g. red, green, blue)
			| ordinal - there is an order to the values (e.g. low, medium, high)
			| continuous - there is an infinite number of values (e.g. 1.2, 1.3, 1.4)
		- depends on number of ways to split
	: how to determine the best attribute to split on? (2 way/binary split | multi-way split)
		- nodes with homogeneous class distribution is preferred
		- need a measure of node impurity
			| entropy - shows randomness of data
			| gini index - shows purity of data 
			| information gain - shows how much information is gained by splitting data
3 determine when to stop splitting
	: when a node is 100% in one class
	: when splitting a node will result in the tree exceeding max depth
	: when info gain is less than threshold
	: when number of examples in node is below threshold
	: early termination

Pros 
	: inexpensive
	: extremely fast
	: easy to interpret
	: accuracy comparable to other methods

Cons
	: can create overly complex trees that do not generalize data well (overfitting)
	: highly sensitive to small changes of the data: high variance
		- small changes in data can result in large changes in tree
		- can be reduced by ensemble methods
------------------------
Review - Decision Trees
- how to determine attribute test condition?
	: attribute types, number of ways to split
- how to determine the best split?
	: homogeneous class distribution, measure of node impurity (entropy, gini index, information gain)

Bias - error between avg model prediction/truth (how well the model fits the data)

Variance - avg variability in model prediction for the dataset (how much the model changes based on the input data)

Bias-Variance Tradeoff
	: high bias, low variance
		- underfitting (overly simplified model)
		- high error on both test and train data
	: low bias, high variance
		- overfitting (overly complex model)
		- low error on train data, high error on test data
	: low bias, low variance
		- good fit
	: high bias, high variance
		- bad fit

Training and Test Errors
- training data: used to train the model
- test data: used to test the model
- test error is more important than training error (because we want to know how well the model generalizes to unseen data)
	: we care abt generalization
	: i.e. training data w no errors seems good but the test data might have a lot of errors bc the model is
		 perfect for the training but not for generalized data sets
- Generalization Errors
	: Re-substition errors: error on training (e(t))
	: Generalization errors: error on test (e'(t))
- Methods for estimating Generalization Errors
	: Optimistic approach (e'(t))
		- use training error as an estimate of generalization error
		- e'(t) = e(t)
		- problem: training error is usually optimistic
	: Pessimistic approach (e'(t))
		- use training error + complexity penalty as an estimate of generalization error
		- e'(t) = e(t) + complexity penalty
		- problem: complexity penalty is usually pessimistic
		e.g. given penality is 0.5
			Training error 10/1000 = 1%
			Generalization error (10 + 30 x 0.5)/1000 = 2.5%
		Optimistic approach (just training error) is too optimistic
		Pessimistic approach (Generalization error) is more realistic
	: Reduced error pruning (REP)
		- split training data into 2 components
			| training set - used to train the model
			| validation set - used to estimate generalization error

Overfitting
- model fits the training data too well
	: model is too complex
	: model is too sensitive to small variations in training data
	: model does not generalize well to test data
- pre-pruning
	: stop growing the tree when some stopping criteria is met
	: e.g. stop growing the tree when the number of examples in a node is below a threshold
- post-pruning
	: grow the tree to completion, then prune the tree
	: e.g. if removing a sub-tree improves generalization error (reduces), then replace sub-tree with leaf node
		- the class label of that leaf node is the majority class of the sub-tree

Model Evaluation in Classification
- metrics for Performance Evaluation
	: predictive capability - how well does the model predict the class label of unseen data?
		- confusion matrix
			| TP - true positive 
			| TN - true negative 
			| FP - false positive (model predicts positive but is actually negative)
			| FN - false negative (model predicts negative but is actually positive)
	: accuracy
		- fraction of correct predictions
		- accuracy = (TP + TN)/(TP + TN + FP + FN) == (true predictions)/(total predictions)
		- can be misleading, e.g. if 99% of data is class A, then a model that always predicts class A will have 99% accuracy
	: precision
		- fraction of positive predictions that are correct
		- precision = TP/(TP + FP) == (true positive predictions)/(total positive predictions)
	: recall
		- fraction of positive examples that are correctly predicted
		- recall = TP/(TP + FN) == (true positive predictions)/(total positive examples)
	: F1 - measure = 2 * (precision * recall)/(precision + recall)
	: ROC curve
		- plots true positive rate (TPR) vs false positive rate (FPR)
- methods for Performance Evaluation
	: estimation
		- holdout method (2/3 data for training, 1/3 data for testing)
			| split data into training and test sets
			| train model on training set
			| evaluate model on test set
		- random subsampling 
			| repeated holdout method
		- cross validation (k-fold cross validation)
			| split data into training and testing
			| training is broken into k equal sized subsets
			| e.g. fold 1 is for validation, fold 2-5 is for training, then fold 2 is for validation, fold 1,3-5 is for training, etc
				- imagine validation data as a mini-test data inside the training data	
			| repeat k times, each time using a different subset for testing
		- bootstrap
			| randomly sample data with replacement
			| train model on sampled data
			| evaluate model on unsampled data

------------------------------------------------
Ensemble Learning
- weak learner/classifier - learning algo that performs slighly better than chance (e.g. correctly labels ~.6)

(Accuracy v Efficiency -------------------------)
| accuracy - true predictions/total predictions |
| efficiency - complexity, time, space			|
(-----------------------------------------------)

- Why use Ensemble Learning?
	: can combine weak learners to create a strong learner
	: basic ensemble functions
		- max voting (classification - discrete)
			| class with the most votes is the predicted class
		- averaging (regression - continuous)
			| average of all predictions is the predicted value
		- weighted averaging (regression)
			| each learner is given a certain weight and thus higher weight have more influence on the final prediction

- What is bagging? (bootstrap aggregating)
	: multiple weak learners are trained in parallel (each learner is independent of each other)
	: each weak learner has input data randomly sampled from the original dataset with replacement
	: final prediction is the average of all weak learners
	: reduces variance (how much the model changes based on input data)
	: reduces overfitting (model fits training data too well)

- What is boosting?
	: multiple weak learners are trained in sequence (each learner is dependent on each other)
		- think of it as a chain of weak learners where each sequential learner has more information than the previous learner (weights of mistakes from prior learner)
	: each subsequent model is trained by giving more weight to examples that were misclassified by previous models
		- e.g. if model 1 misclassified 10 examples, then model 2 will give more weight to those 10 examples
	: final prediction is chosen by scaling each weak-learner's prediction by its weight (why give weight to wrong answers?)
	: reduces bias (error between avg model prediction/truth - how good the model is at predicting the data)

- What is stacking?
	: multiple weak learners (different types) are trained in parallel (each learner is independent of each other)
	: does not use simple voting for final prediction
	: creates a meta learner trained on the outputs of the weak learners
	: 2 subsets of the training data
		- one for training the weak learners
		- one for training the meta learner (weak learners are fed the data then the output is fed to meta learner)

Summary of Ensemble Methods (you can combine based on the problem)
	: bagging - reduces variance, simple voting
	: boosting - reduces bias, weighted voting
	: stacking - improves accuracy, learned voting	

Decision Tree Ensemble Methods
	: issues with decision trees - (over/under fitting)

	Random Forest
		- bagging algo on decision trees
			1. pick N random records from data set (bootstrap)
			2. build a decision tree based on the N records
			3. repeat steps 1-2 k times to make k trees
			4. each tree predicts the class label of the prediction, majority class wins
	Pros
		: used for classification & regression
		: works well w categorical/numerical data, no scaling or transformation required
		: generally provides high accuracy and balance the bias-variance trade off
	Cons
		: not easily interpretable
		: computationally intense
		: like "black box" algo, little control over what model does

	(Bootstrap v Cross Validation (kfold) -------------------------------------)
	| Bootstrap - sampling with replacement (can have overlap/reused data)	   |
	| Cross Validation - sampling without replacement (no overlap/reused data) |
	(--------------------------------------------------------------------------)

	Gradient boosting (how is it different from regular boosting?)
		- Boosting algo on decision trees
			: calculates loss function, updates model parameters in direction that reduces loss function the most
			: final prediction is obtained by adding up the projections of each individual tree
	Pros
		: used for classification & regression
		: helpful for managing complicated heterogenous data and enhancing model correctness
		: can handle categorical and continuous vars
		: high prediction accuracy 
	Cons
	: computationally expensive
	: overfitting

	(Heterogeneous v homogeneous Data set --------------------------------------)
	| Heterogeneous can have different types, e.g. numerical and categorical 	|
	| Homogeneous can only have one type, e.g. all numerical or all categorical |
	(---------------------------------------------------------------------------)

	XGBoost (extreme gradient boosting)
		- Boosting algo on decision trees
			: custom loss function 
			: main diff between Gradient and XG - uses regularization technique
		- performs better than Gradient bc of regularization
		- utilizes CPU parallel processing
		- 2 main components
			: tree boosting - builds trees one at a time, each tree is built to correct the errors of the previous tree
			: regularization - penalizes more complex models

	AdaBoost (Adaptive Boosting)
		- Decision Stumps (decision trees with only 1 split)
		- weak learner needs to minimize weighted error
		- more intuitive than Gradient but Gradient is more flexible (generic algo)
		- AdaBoost is slower bc needing multiple iterations to build the sequence of models
		- AdaBoost is more abt "voting weights" (?)
		- Gradient is more abt "adding gradient optimization" (?)

Summary
	: bagging 
		- cut training data into subsets (one subset per learner)
		- train weak learners in parallel (each their own subset)
	: boosting
		- entire training data per learner
		- weak learners are trained in sequence, subsequent learner takes weights from previous learner's mistakes
	: stacking
		- 2 subsets of training data (one for learners, one for meta learner)
			: learners are trained in parallel (training)
			: meta learner is trained on the outputs of the learners (trained)
	------------------------
	new types
		: random forest - bagging algo on decision trees
			- 
		: gradient boosting - boosting algo on decision trees
		: XGBoost - extreme gradient boosting
		: AdaBoost - adaptive boosting
------------------------
Review
- Supervised Learning - given training data + desired outputs (labels)
	: regression - y is a real number
	: classification - y is categorical
- Ensemble Learning - combining multiple weak learners to create a strong learner
	: bagging - reduces variance, simple voting
	: boosting - reduces bias, weighted voting
	: stacking - improves accuracy, learned voting

------------------------
Parametric ML algo - summarizes data w set of parameters of a fixed size
	: 2 steps
		1. select a form for the function
		2. learn coefficients for the function from training data
	: examples - linear regression, logistic regression, linear discriminant analysis, naive bayes, simple neural networks
	Pros
		: simple (easy to understand/interpret)
		: fast (when learning from data)
		: less training data required
	Cons
		: less flexible (constrained to specific form )
		: limited complexity (more suited to simpler problems)
		: poor fit (unlikely to match true underlying function [imagine a linear function trying to fit a quadratic function])

Non-parametric ML algo - does not summarize data w set of parameters of a fixed 
	: good when a lot of data and no prior knowledge
	: when you don't want to worry abt choosing the right features
	: seek best fit to training data
		- constructing the mapping function while maintaining the ability to generalize unseen data
		- e.g. K-Nearest Neighbors (KNN), decision trees, support vector machines
	Pros
		: flexible (not constrained to specific form)
		: powerful (no assumption abt underlying function)
		: performance (high perf models for prediction)
	Cons
		: slow (when learning from data)
		: more training data required
		: overfitting (more likely to overfit training data)

( try drawing the circles with each algo inside each to see how they relate )-----------------

K-Nearest Neighbors (KNN) [non-parametric supervised learning]
- one of the simplest ML classifier algo
- Simple Idea: label a new pt the same as closest known pt
- type of Instance-Based learning (memory based)

How does KNN work?
	: given set of labeled pts (training data) and a new pt (test data)
	: find k closest pts in training data and label new pt accordingly
		- e.g. given 2 classes (red, blue) and k = 3
			| 2 red pts, 1 blue pt
			| new pt is red bc 2/3 closest pts are red (if all classes [colors] given equal weight)
	
Who gets to vote?
	: "national" election, majority rules and most numerous class wins
		- no universal suffrage for k-NN
	: "local" election used for k-NN, nearest k pts

How to count (weight) the votes?
	: weight by distance
		- closer pts have more influence on the prediction
		- weight is proportional to the inverse of the distance (1/distance(k pt))
		- sum inverse distances together to get weight of class (e.g. 1/r1 + 1/r2 vs 1/b1)
		- Distance metrics/formulas
			| Euclidean Distance - straight line distance
			| Manhattan Distance - sum of absolute differences
			| Minkowski Distance - generalized distance metric of both Euclidean and Manhattan
			| Cosine Distance - angle between vectors
			| Jaccard Distance - similarity between sets
	: weight by class frequency (use all pts in the graph)
		- e.g. 2 red, 1 blue in k = 3
			| 10 red total, 4 blue total
	: weight by fixed radius (pts within radius are given weight, pts outside radius are not given weight)
		- the mode (most common class) is the predicted class
		- good for sparse data
	
How to choose K in KNN?
	: based on input data
	: if input has outliers or noise, higher is better
	: cross-validation (k-fold) to find optimal k

KNN - Pros
	: no training required
	: computations deferred to scoring phase
	: as training size grows, accuracy increases
	: works for multi-class problems
		- e.g. given 3 classes (red, blue, green)
	: easy to implement (low complexity)

KNN - Cons
	: scoring is not straightforward
		- calculate all distances to all points needed for score computation
	: very sensitive to local structure of data
		- e.g. if data is clustered, then KNN will be biased towards the cluster
	: does not scale well (time consuming to calculate distances if data set is large)
	: Curse of Dimensionality
		- hard time classifying data pts when dimensionality is too high
		- dimensionality: number of features
	: prone to overfitting

KNN - Summary
	: non-parametric supervised learning
	: simple
	: lazy learning (no training required)
	: can be combined w other techniques like PCA 
	